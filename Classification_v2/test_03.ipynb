{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports\n",
    "'''\n",
    "import h5py\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paths\n",
    "'''\n",
    "TRAINED_MODEL_PATH = ''\n",
    "TEST_HDF5_PATH = ''\n",
    "SUBMISSION_FILE_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Constants\n",
    "'''\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset and Dataloader\n",
    "'''\n",
    "data_transforms_album = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Downscale(p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.15, \n",
    "                           rotate_limit=60, \n",
    "                           p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}\n",
    "\n",
    "class ISIC2024_HDF5_ALBUM(Dataset):\n",
    "    '''\n",
    "    With augmentations using albumentations \n",
    "    '''\n",
    "    def __init__(self, hdf5_path, annotations_df=None, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.annotations_df = annotations_df\n",
    "        self.transform = transform\n",
    "        self.image_ids = []\n",
    "        \n",
    "        self.hdf5_file = h5py.File(self.hdf5_path, 'r')\n",
    "\n",
    "        if self.annotations_df is not None:\n",
    "            self.image_ids = annotations_df['isic_id']\n",
    "            self.labels = annotations_df.set_index('isic_id')['target'].to_dict()\n",
    "        else:\n",
    "            self.image_ids = list(self.hdf5_file.keys())\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = np.array(Image.open(BytesIO(self.hdf5_file[image_id][()])))\n",
    "        \n",
    "        # image = self.load_image(self.hdf5_file[image_id][()])\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image)\n",
    "            image = self.transform(image=image)[\"image\"]        # Albumentations returns a dictionary with keys like 'image', 'mask', etc., depending on the transformations applied.\n",
    "\n",
    "        # Check for NaN in image\n",
    "        if torch.isnan(image).any():\n",
    "            print(f\"NaN detected in image {image_id}\")\n",
    "\n",
    "        if self.annotations_df is not None:\n",
    "            label = self.labels[image_id]\n",
    "            # Check for NaN in label\n",
    "            if np.isnan(label):\n",
    "                print(f\"NaN detected in label for image {image_id}\")\n",
    "            return image, label, image_id\n",
    "        else:\n",
    "            return image, image_id\n",
    "        \n",
    "    # def load_image(self, image_data):\n",
    "    #     # Decode the image data from HDF5 file using OpenCV\n",
    "    #     image = cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "    #     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    #     # image = np.transpose(image, (1, 2, 0))  # Convert HxWxC to CxHxW\n",
    "    #     return image\n",
    "    \n",
    "    def close(self):\n",
    "        self.hdf5_file.close()\n",
    "\n",
    "\n",
    "'''\n",
    "DataLoader\n",
    "'''\n",
    "def get_loader(test_hdf5_path, \n",
    "               train_labels_df = None, \n",
    "               train_hdf5_path = None, \n",
    "               dataset_cls=ISIC2024_HDF5_ALBUM,\n",
    "               train_img_trans=data_transforms_album[\"train\"], \n",
    "               test_img_trans=data_transforms_album[\"test\"], \n",
    "               batch=32, \n",
    "               seed=None):\n",
    "    if train_labels_df is not None and train_hdf5_path is not None:\n",
    "        train_dataset_all = dataset_cls(hdf5_path=train_hdf5_path, annotations_df=train_labels_df, transform=train_img_trans)\n",
    "        test_dataset = dataset_cls(hdf5_path=test_hdf5_path, transform=test_img_trans)\n",
    "\n",
    "        train_annotations_all = train_labels_df\n",
    "        labels = train_annotations_all['target']\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        train_idx, val_idx = next(splitter.split(train_annotations_all, labels))\n",
    "        train_subset = Subset(train_dataset_all, train_idx)\n",
    "        val_subset = Subset(train_dataset_all, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        test_dataset = dataset_cls(hdf5_path=test_hdf5_path, transform=test_img_trans)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "    \n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_save_path = None, imagenet_weights_path = None):   # Use this for submission rather than load_checkpoint() defined above\n",
    "    '''\n",
    "    To load model during evaluation on test set\n",
    "    '''\n",
    "    if model_save_path:\n",
    "        model = torchvision.models.resnet34(weights=None)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(in_features=num_ftrs, out_features=1)\n",
    "        model.load_state_dict(torch.load(model_save_path)[\"state_dict\"])\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = torchvision.models.resnet34(weights=None)\n",
    "        model.load_state_dict(torch.load(imagenet_weights_path))\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features=num_ftrs, out_features=1)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def create_submission(model, test_loader, submission_file_path):\n",
    "    '''\n",
    "    To predict class probabilities on test data and generate submission.csv file\n",
    "    '''\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, image_names in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "            image_ids.extend(image_names)  # Append all image names from the batch\n",
    "\n",
    "    # Check if the lengths match\n",
    "    if len(image_ids) != len(predictions):\n",
    "        print(f\"Warning: Number of image IDs ({len(image_ids)}) does not match number of predictions ({len(predictions)})\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'isic_id': image_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_save_path = TRAINED_MODEL_PATH)\n",
    "test_loader = get_loader(test_hdf5_path=TEST_HDF5_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(model, test_loader, submission_file_path=SUBMISSION_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
