{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import os, time\n",
    "import csv, numpy as np, pandas as pd, h5py\n",
    "from tqdm import tqdm\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_pytorch import ViT\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Constants'''\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "CLASSES = 1\n",
    "EPOCHS = 50\n",
    "MIN_EPOCH_TRAIN = 10\n",
    "PATIENCE = 5\n",
    "EPSILON = 0.0005\n",
    "NEG_POS_RATIO = 20\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Paths'''\n",
    "TRAIN_HDF5_PATH = \"/kaggle/input/isic-2024-challenge/train-image.hdf5\"\n",
    "TEST_HDF5_PATH = \"/kaggle/input/isic-2024-challenge/test-image.hdf5\"\n",
    "ANNOTATIONS_FILE = \"/kaggle/input/isic-2024-challenge/train-metadata.csv\"\n",
    "MODEL_SAVE_PATH_ = \"/kaggle/working/\"\n",
    "LOG_FILE_1 = \"/kaggle/working/\"\n",
    "LOG_FILE_2 = \"/kaggle/working/log_folds.csv\"\n",
    "SUBMISSION_FILE_PATH = \"/kaggle/working/submission.csv\"\n",
    "METRICS_PLOT_SAVE_PATH = \"/kaggle/working/metrics.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset and Dataloaders'''\n",
    "data_transforms_v2 = {\n",
    "    \"train\": v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.ToImage(),\n",
    "        v2.RandomRotation(degrees=(0, 360)),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.ToDtype(torch.float32, scale = True)\n",
    "    ]),\n",
    "    \"test\": v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale = True)\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_transforms_album = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(224, 224, interpolation=cv2.INTER_AREA),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Downscale(p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=60, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224,0.225], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "        A.Resize(224, 224, interpolation=cv2.INTER_AREA),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2()], p=1.)\n",
    "}\n",
    "\n",
    "class ISIC2024_HDF5(Dataset):\n",
    "    def __init__(self, hdf5_path, annotations_df=None, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.annotations_df = annotations_df\n",
    "        self.transform = transform\n",
    "        self.image_ids = []\n",
    "        self.hdf5_file = h5py.File(self.hdf5_path, 'r')\n",
    "        if self.annotations_df is not None:\n",
    "            self.image_ids = annotations_df['isic_id']\n",
    "            self.labels = annotations_df.set_index('isic_id')['target'].to_dict()\n",
    "        else:\n",
    "            self.image_ids = list(self.hdf5_file.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = Image.open(BytesIO(self.hdf5_file[image_id][()]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if torch.isnan(image).any():\n",
    "            print(f\"NaN detected in image {image_id}\")\n",
    "        if self.annotations_df is not None:\n",
    "            label = self.labels[image_id]\n",
    "            if np.isnan(label):\n",
    "                print(f\"NaN detected in label for image {image_id}\")\n",
    "            return image, label, image_id\n",
    "        else:\n",
    "            return image, image_id\n",
    "    \n",
    "    def close(self):\n",
    "        self.hdf5_file.close()\n",
    "\n",
    "class ISIC2024_HDF5_ALBUM(Dataset):\n",
    "    def __init__(self, hdf5_path, annotations_df=None, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.annotations_df = annotations_df\n",
    "        self.transform = transform\n",
    "        self.image_ids = []\n",
    "        self.hdf5_file = h5py.File(self.hdf5_path, 'r')\n",
    "        if self.annotations_df is not None:\n",
    "            self.image_ids = annotations_df['isic_id']\n",
    "            self.labels = annotations_df.set_index('isic_id')['target'].to_dict()\n",
    "        else:\n",
    "            self.image_ids = list(self.hdf5_file.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = np.array(Image.open(BytesIO(self.hdf5_file[image_id][()])))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        if torch.isnan(image).any():\n",
    "            print(f\"NaN detected in image {image_id}\")\n",
    "        if self.annotations_df is not None:\n",
    "            label = self.labels[image_id]\n",
    "            if np.isnan(label):\n",
    "                print(f\"NaN detected in label for image {image_id}\")\n",
    "            return image, label, image_id\n",
    "        else:\n",
    "            return image, image_id\n",
    "    \n",
    "    def close(self):\n",
    "        self.hdf5_file.close()\n",
    "\n",
    "def get_loader(test_hdf5_path, \n",
    "               train_labels_df = None, \n",
    "               train_hdf5_path = None, \n",
    "               dataset_cls=ISIC2024_HDF5_ALBUM,\n",
    "               train_img_trans=data_transforms_album[\"train\"], \n",
    "               test_img_trans=data_transforms_album[\"test\"], \n",
    "               batch=32, \n",
    "               seed=None):\n",
    "    if train_labels_df is not None and train_hdf5_path is not None:\n",
    "        train_dataset_all = dataset_cls(hdf5_path=train_hdf5_path, annotations_df=train_labels_df, transform=train_img_trans)\n",
    "        test_dataset = dataset_cls(hdf5_path=test_hdf5_path, transform=test_img_trans)\n",
    "        train_annotations_all = train_labels_df\n",
    "        labels = train_annotations_all['target']\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        train_idx, val_idx = next(splitter.split(train_annotations_all, labels))\n",
    "        train_subset = Subset(train_dataset_all, train_idx)\n",
    "        val_subset = Subset(train_dataset_all, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        test_dataset = dataset_cls(hdf5_path=test_hdf5_path, transform=test_img_trans)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Utils'''\n",
    "def train(epochs, model, learning_rate, train_dl, val_dl, min_epoch_train, patience, epsilon, log_file, model_save_path, criterion = nn.BCEWithLogitsLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val_pauc = -1.0\n",
    "    current_patience = 0\n",
    "    with open(log_file, \"w\", newline=\"\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['Epoch', 'Learning Rate', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Validation Precision', 'Validation Recall', 'Validation F1 Score', 'Validation pAUC'])\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n | Epoch: {epoch+1}\")\n",
    "            total_loss = 0\n",
    "            num_corr = 0\n",
    "            num_samp = 0\n",
    "            loop = tqdm(train_dl)\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, labels, _) in enumerate(loop):\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs).squeeze(1)\n",
    "                    loss = criterion(outputs, labels.float())\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"NaN loss detected at batch {batch_idx}\")\n",
    "                    continue\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                num_corr += ((preds > 0.5) == labels).sum()\n",
    "                num_samp += preds.size(0)\n",
    "                total_loss += loss.item()\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "            avg_loss = total_loss / len(train_dl)\n",
    "            acc = num_corr / num_samp\n",
    "            print(f\"| Epoch {epoch+1}/{epochs} total training loss: {total_loss}, average training loss: {avg_loss}.\")\n",
    "            print(\"On Validation Data:\")\n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                val_loss, val_acc, val_pre, val_rec, val_f1, val_pauc = evaluate(val_dl, model, criterion)\n",
    "            print(\"learning rate:\", scheduler.get_last_lr()[0])\n",
    "            row = [epoch+1, scheduler.get_last_lr()[0], avg_loss, acc.item(), val_loss, val_acc, val_pre, val_rec, val_f1, val_pauc]\n",
    "            csv_writer.writerow(row)\n",
    "            if epoch + 1 > min_epoch_train:\n",
    "                if val_pauc > best_val_pauc and (val_pauc - best_val_pauc) > epsilon:\n",
    "                    best_val_pauc = val_pauc\n",
    "                    print(f'Validation pAUC improved by more than {epsilon}, ({best_val_pauc} > {best_val_pauc})); saving model...')\n",
    "                    checkpoint = {\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    }\n",
    "                    save_checkpoint(checkpoint, model_save_path)\n",
    "                    print(f'Model saved at {model_save_path}')\n",
    "                    current_patience = 0\n",
    "                else:\n",
    "                    current_patience += 1\n",
    "                    print(f'Validation pAUC did not improve. Patience left: {patience - current_patience}')\n",
    "                    if current_patience >= patience:\n",
    "                        print(f'\\n---Early stopping at epoch {epoch+1}.---')\n",
    "                        break\n",
    "            else:\n",
    "                if val_pauc > best_val_pauc:\n",
    "                    best_val_pauc = val_pauc\n",
    "                    checkpoint = {\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    }\n",
    "                    save_checkpoint(checkpoint, model_save_path)\n",
    "                    print(f'Model saved at {model_save_path}')\n",
    "            print(f'Current Best Validation pAUC: {best_val_pauc}')\n",
    "            scheduler.step()\n",
    "    print('Training complete.')\n",
    "    return best_val_pauc\n",
    "\n",
    "def pauc_above_tpr(y_true, y_pred, min_tpr=0.80):\n",
    "    y_true = abs(np.array(y_true) - 1)\n",
    "    y_pred = -1.0 * np.array(y_pred)\n",
    "    if np.isnan(y_true).any() or np.isnan(y_pred).any():\n",
    "        print(\"NaN values detected in inputs to pauc_above_tpr\")\n",
    "        return 0\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    max_fpr = 1 - min_tpr\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    if len(fpr) < 2:\n",
    "        print(\"Warning: Not enough points to compute pAUC. Returning 0.\")\n",
    "        return 0\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "    return partial_auc\n",
    "\n",
    "def evaluate(loader, model, criterion):\n",
    "    metric = BinaryF1Score(threshold=0.5).to(DEVICE)\n",
    "    prec = BinaryPrecision(threshold=0.5).to(DEVICE)\n",
    "    recall = BinaryRecall(threshold=0.5).to(DEVICE)\n",
    "    acc = BinaryAccuracy(threshold=0.5).to(DEVICE)\n",
    "    loss = 0.0\n",
    "    num_corr = 0\n",
    "    num_samp = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(\"NaN detected in model outputs\")\n",
    "                continue\n",
    "            loss += criterion(outputs, labels.float()).item()\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            num_corr += ((preds > 0.5) == labels).sum()\n",
    "            num_samp += preds.size(0)\n",
    "            metric.update(preds, labels)\n",
    "            prec.update(preds, labels)\n",
    "            recall.update(preds, labels)\n",
    "            acc.update(preds, labels)\n",
    "            all_preds.extend(preds.cpu().detach().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = loss / len(loader)\n",
    "    accu = float(num_corr) / float(num_samp)\n",
    "    pauc = pauc_above_tpr(all_labels, all_preds)\n",
    "    print(f\"Total loss: {loss}, Average loss: {avg_loss}\")\n",
    "    print(f\"Got {num_corr}/{num_samp} correct with accuracy {accu*100:.2f}\")\n",
    "    print(f\"pAUC above 80% TPR: {pauc:.3f}, Accuracy: {acc.compute().item():.3f}, precision: {prec.compute().item():.3f}, recall: {recall.compute().item():.3f}, F1Score: {metric.compute().item():.3f}\")\n",
    "    model.train()\n",
    "    return avg_loss, acc.compute().item(), prec.compute().item(), recall.compute().item(), metric.compute().item(), pauc\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def load_model(model_save_path = None):\n",
    "    if model_save_path:\n",
    "        model = ViT(\n",
    "            image_size = 224,\n",
    "            patch_size = 16,\n",
    "            num_classes = 1,\n",
    "            dim = 768,\n",
    "            depth = 12,\n",
    "            heads = 12,\n",
    "            mlp_dim = 3072,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        model.load_state_dict(torch.load(model_save_path)[\"state_dict\"])\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "    else:\n",
    "        model = ViT(\n",
    "            image_size = 224,\n",
    "            patch_size = 16,\n",
    "            num_classes = 1,\n",
    "            dim = 768,\n",
    "            depth = 12,\n",
    "            heads = 12,\n",
    "            mlp_dim = 3072,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def create_submission(model, test_loader, submission_file_path):\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, image_names in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "            image_ids.extend(image_names)\n",
    "    if len(image_ids) != len(predictions):\n",
    "        print(f\"Warning: Number of image IDs ({len(image_ids)}) does not match number of predictions ({len(predictions)})\")\n",
    "    submission_df = pd.DataFrame({\n",
    "        'isic_id': image_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_file_path}\")\n",
    "\n",
    "def visualize_train_images(images, titles=None):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        if titles:\n",
    "            plt.title(f\"{titles[0][i]} (label: {titles[1][i]})\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_test_images(images, titles=None):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_from_files(file_paths, save_path=None):\n",
    "    num_files = len(file_paths)\n",
    "    rows = 3\n",
    "    cols = num_files\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10 * cols, 10))\n",
    "    if num_files == 1:\n",
    "        axes = axes[:, None]\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        df = pd.read_csv(file_path)\n",
    "        epochs = df['Epoch']\n",
    "        learning_rate = df['Learning Rate']\n",
    "        train_loss = df['Training Loss']\n",
    "        valid_loss = df['Validation Loss']\n",
    "        valid_pAUC = df['Validation pAUC']\n",
    "        axes[0, i].plot(epochs, train_loss, label='Training Loss', marker='o')\n",
    "        axes[0, i].plot(epochs, valid_loss, label='Validation Loss', marker='o')\n",
    "        axes[0, i].set_title(f'File: {file_path.split(\"/\")[-1]}')\n",
    "        axes[0, i].set_xlabel('Epochs')\n",
    "        axes[0, i].set_ylabel('Loss')\n",
    "        axes[0, i].legend()\n",
    "        axes[1, i].plot(epochs, learning_rate, label='Learning Rate', marker='o', color='orange')\n",
    "        axes[1, i].set_title(f'File: {file_path.split(\"/\")[-1]}')\n",
    "        axes[1, i].set_xlabel('Epochs')\n",
    "        axes[1, i].set_ylabel('Learning Rate')\n",
    "        axes[1, i].legend()\n",
    "        axes[2, i].plot(epochs, valid_pAUC, label='Validation pAUC', marker='o', color='green')\n",
    "        axes[2, i].set_title(f'File: {file_path.split(\"/\")[-1]}')\n",
    "        axes[2, i].set_xlabel('Epochs')\n",
    "        axes[2, i].set_ylabel('Validation pAUC')\n",
    "        axes[2, i].legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training loop'''\n",
    "def lesgooo():\n",
    "    annotations_df_full = pd.read_csv(ANNOTATIONS_FILE, low_memory=False)\n",
    "    df_positive_all = annotations_df_full[annotations_df_full[\"target\"] == 1].reset_index(drop=True)\n",
    "    df_negative_all = annotations_df_full[annotations_df_full[\"target\"] == 0].reset_index(drop=True)\n",
    "    df_negative_trunc = df_negative_all.sample(df_positive_all.shape[0]*NEG_POS_RATIO)\n",
    "    annotations_df_trunc = pd.concat([df_positive_all, df_negative_trunc]).sample(frac=1).reset_index()\n",
    "    val_pAUC = []\n",
    "    with open(LOG_FILE_2, 'w', newline=\"\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['Fold', \"Model_Name\", \"val_pAUC\"])\n",
    "        for fold in range(FOLDS):\n",
    "            train_dl, val_dl, _ = get_loader(train_labels_df=annotations_df_trunc,\n",
    "                                             train_hdf5_path=TRAIN_HDF5_PATH,\n",
    "                                             test_hdf5_path=TEST_HDF5_PATH)\n",
    "            model_vit = load_model()\n",
    "            print(f\"---------------\\nTraining for fold: {fold+1}:\\n---------------\")\n",
    "            val_pAUC_fold = train(epochs=EPOCHS,\n",
    "                                        model=model_vit,\n",
    "                                        learning_rate=LEARNING_RATE,\n",
    "                                        train_dl=train_dl,\n",
    "                                        val_dl=val_dl,\n",
    "                                        min_epoch_train=MIN_EPOCH_TRAIN,\n",
    "                                        patience=PATIENCE,\n",
    "                                        epsilon=EPSILON,\n",
    "                                        log_file=os.path.join(LOG_FILE_1, f'log_vit_aug_fold_{fold}.csv'),\n",
    "                                        model_save_path=os.path.join(MODEL_SAVE_PATH_, f'model_vit_aug_fold_{fold}.pth'))\n",
    "            val_pAUC.append(val_pAUC_fold)\n",
    "            csv_writer.writerow([fold, os.path.basename(os.path.join(MODEL_SAVE_PATH_, f'model_vit_aug_fold_{fold}.pth')), val_pAUC_fold])\n",
    "    best_model_fold_index = val_pAUC.index(max(val_pAUC))\n",
    "    print(f\"Average of OOF pAUC: {np.mean(val_pAUC)}\")\n",
    "    file_paths = [os.path.join(LOG_FILE_1, f'log_vit_aug_fold_{i}.csv') for i in range(FOLDS)]\n",
    "    plot_metrics_from_files(file_paths, save_path=METRICS_PLOT_SAVE_PATH)\n",
    "    \n",
    "    return best_model_fold_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = lesgooo()\n",
    "print(f\"\\n\\nDone, index = {IDX}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = f\"/kaggle/working/model_vit_aug_fold_{IDX}.pth\"\n",
    "print(f\"Best Model: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluation'''\n",
    "def predict():\n",
    "    model = load_model(model_save_path=MODEL_SAVE_PATH)\n",
    "    test_loader = get_loader(test_hdf5_path=TEST_HDF5_PATH)\n",
    "    create_submission(model, test_loader, submission_file_path=SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
